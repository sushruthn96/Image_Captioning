{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datasets/COCO-2015/train2014'\n",
    "annotation_file = 'annotations/captions_train2014.json'\n",
    "val_dir = '/datasets/COCO-2015/val2014'\n",
    "val_annotation_file = 'annotations/captions_val2014.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.58s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import nbimporter\n",
    "import data_loader as data_loader\n",
    "import pickle\n",
    "from vocabbuild import Vocabulary\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as td\n",
    "\n",
    "with open(\"vocab.pkl\", 'rb') as fi:\n",
    "    vocabulary = pickle.load(fi)\n",
    "\n",
    "image_size = (229,229)\n",
    "transform = transforms.Compose([ \n",
    "        # Try resize\n",
    "        #transforms.RandomCrop(crop_size), \n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(), \n",
    "        # Using Imagenet std and mean\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])   \n",
    "\n",
    "coco_train = data_loader.COCODataset(root = data_dir, annFile = annotation_file, \n",
    "                                     vocabulary=vocabulary, transform = transform)\n",
    "train_loader = td.DataLoader(coco_train, batch_size = 1, shuffle = True,\n",
    "                                         pin_memory = True, collate_fn = data_loader.coco_batch)\n",
    "\n",
    "coco_val = data_loader.COCODataset(root = val_dir, annFile = val_annotation_file, \n",
    "                                     vocabulary=vocabulary, transform = transform)\n",
    "val_loader = td.DataLoader(coco_val, batch_size = 1, shuffle = False,\n",
    "                                         pin_memory = True, collate_fn = data_loader.coco_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check train_loader\n",
    "# for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "#     if (i > 4):\n",
    "#         break\n",
    "#     data_loader.myimshow(images[0])\n",
    "#     cap_string = \"\"\n",
    "#     for j in list(captions.data.numpy()[0]):\n",
    "#         cap_string += vocabulary.idx2word[j] + \" \"        \n",
    "#     print(cap_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot functions\n",
    "import matplotlib.pyplot as plt\n",
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    image = (image + 1) / 2\n",
    "    image[image < 0] = 0\n",
    "    image[image > 1] = 1\n",
    "    h = ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    return h\n",
    "\n",
    "def plot(exp, fig, axes):\n",
    "    axes[0].clear()\n",
    "    axes[1].clear()\n",
    "    axes[0].plot([exp.history[k][0]['loss'] for k in range(exp.epoch)],\n",
    "    label=\"training loss\")\n",
    "    axes[0].plot([exp.history[k][1]['loss'] for k in range(exp.epoch)],\n",
    "    label=\"evaluation loss\")\n",
    "    axes[1].plot([exp.history[k][0]['accuracy'] for k in range(exp.epoch)],\n",
    "    label=\"training accuracy\")\n",
    "    axes[1].plot([exp.history[k][1]['accuracy'] for k in range(exp.epoch)],\n",
    "    label=\"evaluation accuracy\")\n",
    "    axes[1].set_xlabel(\"epochs\")\n",
    "    axes[0].set_xlabel(\"epochs\")\n",
    "    axes[1].set_ylabel(\"accuracy\")\n",
    "    axes[0].set_ylabel(\"loss\")\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weight_initialization\n",
    "weight_matrix = weight_initialization.get_weight_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nntools as nt\n",
    "class NNClassifier(nt.NeuralNetwork):\n",
    "    def __init__(self):\n",
    "        super(NNClassifier, self).__init__()\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "    def criterion(self, y, d):\n",
    "        # MSE loss implementation\n",
    "        return self.classification_loss(y, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decoder network\n",
    "from torch import nn\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class LSTM_custom(NNClassifier):\n",
    "    def __init__(self, weights_matrix, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(LSTM_custom, self).__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "    \n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embedding(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the models\n",
    "import models_custom\n",
    "encoder = models_custom.CNN(out_classes = 300).to(device)\n",
    "decoder = LSTM_custom(weights_matrix = weight_matrix, hidden_size = 512, \n",
    "                             vocab_size = weight_matrix.shape[0], num_layers = 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationStatsManager(nt.StatsManager):\n",
    "    def __init__(self):\n",
    "        super(ClassificationStatsManager, self).__init__()\n",
    "    def init(self):\n",
    "        super(ClassificationStatsManager, self).init()\n",
    "        self.running_accuracy = 0\n",
    "    def accumulate(self, loss, x, y, d):\n",
    "        super(ClassificationStatsManager, self).accumulate(loss, x, y, d)\n",
    "        _, l = torch.max(y, 1)\n",
    "        self.running_accuracy += torch.mean((l == d).float())\n",
    "    def summarize(self):\n",
    "        loss = super(ClassificationStatsManager, self).summarize()\n",
    "        accuracy = (100 * self.running_accuracy)/self.number_update\n",
    "        return {'loss': loss, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "adam = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "stats_manager = ClassificationStatsManager()\n",
    "exp1 = nt.Experiment(decoder, train_loader, val_loader, adam, stats_manager,\n",
    "output_dir=\"captioning\", perform_validation_during_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run() got an unexpected keyword argument 'encoder_net'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8dbeca1be6dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: run() got an unexpected keyword argument 'encoder_net'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(7, 3))\n",
    "exp1.run(encoder_net=encoder, num_epochs=20, plot=lambda exp: plot(exp, fig=fig, axes=axes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
