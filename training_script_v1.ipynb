{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datasets/COCO-2015/train2014'\n",
    "annotation_file = 'annotations/captions_train2014.json'\n",
    "val_dir = '/datasets/COCO-2015/val2014'\n",
    "val_annotation_file = 'annotations/captions_val2014.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.96s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.34s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import nbimporter\n",
    "import data_loader as data_loader\n",
    "import pickle\n",
    "from vocabbuild import Vocabulary\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as td\n",
    "\n",
    "with open(\"vocab.pkl\", 'rb') as fi:\n",
    "    vocabulary = pickle.load(fi)\n",
    "\n",
    "image_size = (229,229)\n",
    "transform = transforms.Compose([ \n",
    "        # Try resize\n",
    "        #transforms.RandomCrop(crop_size), \n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(), \n",
    "        # Using Imagenet std and mean\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])   \n",
    "\n",
    "coco_train = data_loader.COCODataset(root = data_dir, annFile = annotation_file, \n",
    "                                     vocabulary=vocabulary, transform = transform)\n",
    "train_loader = td.DataLoader(coco_train, batch_size = 64, shuffle = True,\n",
    "                                         pin_memory = True, collate_fn = data_loader.coco_batch)\n",
    "\n",
    "coco_val = data_loader.COCODataset(root = val_dir, annFile = val_annotation_file, \n",
    "                                     vocabulary=vocabulary, transform = transform)\n",
    "val_loader = td.DataLoader(coco_val, batch_size = 64, shuffle = False,\n",
    "                                         pin_memory = True, collate_fn = data_loader.coco_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check train_loader\n",
    "# for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "#     if (i > 4):\n",
    "#         break\n",
    "#     data_loader.myimshow(images[0])\n",
    "#     cap_string = \"\"\n",
    "#     for j in list(captions.data.numpy()[0]):\n",
    "#         cap_string += vocabulary.idx2word[j] + \" \"        \n",
    "#     print(cap_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot functions\n",
    "import matplotlib.pyplot as plt\n",
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    image = (image + 1) / 2\n",
    "    image[image < 0] = 0\n",
    "    image[image > 1] = 1\n",
    "    h = ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    return h\n",
    "\n",
    "def plot(exp, fig, axes, noisy, visu_rate=2):\n",
    "    if exp.epoch % visu_rate != 0:\n",
    "        return\n",
    "    with torch.no_grad():\n",
    "        denoised = exp.net(noisy[np.newaxis].to(exp.net.device))[0]\n",
    "    axes[0][0].clear()\n",
    "    axes[0][1].clear()\n",
    "    axes[1][0].clear()\n",
    "    axes[1][1].clear()\n",
    "    \n",
    "    myimshow(noisy, ax=axes[0][0])\n",
    "    axes[0][0].set_title('Noisy image')\n",
    "    \n",
    "    axes[0][1].set_title('Denoised image')\n",
    "    myimshow(denoised, ax=axes[0][1])\n",
    "    \n",
    "    axes[1][0].plot([exp.history[k][0]['loss'] for k in range(exp.epoch)],label=\"training loss\")\n",
    "    axes[1][1].plot([exp.history[k][0]['psnr'] for k in range(exp.epoch)],label=\"training psnr\")\n",
    "    \n",
    "    axes[1][0].set_xlabel(\"epochs\")\n",
    "    axes[1][1].set_xlabel(\"epochs\")\n",
    "    axes[1][0].set_ylabel(\"loss\")\n",
    "    axes[1][1].set_ylabel(\"psnr\")\n",
    "    axes[1][0].legend()\n",
    "    axes[1][1].legend()\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weight_initialization\n",
    "weight_matrix = weight_initialization.get_weight_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define decoder network\n",
    "from torch import nn\n",
    "\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': torch.from_numpy(weights_matrix)})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class LSTM_custom(nn.Module):\n",
    "    def __init__(self, weights_matrix, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(LSTM_custom, self).__init__()\n",
    "        embed_size = 256\n",
    "        #self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "    \n",
    "    def forward(self, features, captions, lengths):\n",
    "        embeddings = self.embedding(captions)\n",
    "        print(embeddings.shape)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        print(embeddings.shape)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        #print(packed)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        print(hiddens[0].shape)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        print(outputs.shape)\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embedding(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# Build the models\n",
    "import models_custom\n",
    "encoder = models_custom.CNN(out_classes = 300).to(device)\n",
    "decoder = LSTM_custom(weights_matrix = weight_matrix, hidden_size = 512, \n",
    "                             vocab_size = weight_matrix.shape[0], num_layers = 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 92, 300])\n",
      "torch.Size([64, 93, 300])\n",
      "PackedSequence(data=tensor([[ 0.5099, -0.4875,  0.2736,  ..., -0.0274,  0.1766,  0.4399],\n",
      "        [ 0.5031,  0.1621,  0.0188,  ..., -0.1652, -0.4765,  0.6706],\n",
      "        [ 0.6448,  0.1570, -0.0433,  ...,  0.1785,  0.1674,  0.1571],\n",
      "        ...,\n",
      "        [ 0.3514, -0.1685,  0.1196,  ..., -0.3544, -0.4076,  0.4999],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786]],\n",
      "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 63, 62, 60, 60, 59, 59, 57, 53, 46, 40, 40, 37, 34,\n",
      "        30, 20, 20, 15, 13, 12, 10,  9,  8,  6,  6,  6,  5,  3,  3,  3,  3,  3,\n",
      "         2,  1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([4624, 512])\n",
      "torch.Size([4624, 9956])\n",
      "torch.Size([4624, 9956]) torch.Size([4624])\n",
      "[ 0.00216774 -0.02547548  0.07766122 ... -0.00961169  0.02473936\n",
      "  0.00404553] 1\n",
      "Epoch [0/25], Step [0/1294], Loss: 9.2176, Perplexity: 10072.7919\n",
      "torch.Size([64, 87, 300])\n",
      "torch.Size([64, 88, 300])\n",
      "PackedSequence(data=tensor([[ 0.5208,  0.5035, -0.1364,  ...,  0.0090, -0.7577,  0.3550],\n",
      "        [-0.0057, -0.1876, -0.4522,  ...,  0.3960, -0.4421,  0.4071],\n",
      "        [ 0.8904, -0.0735,  0.2077,  ...,  0.4643,  0.2698, -0.3032],\n",
      "        ...,\n",
      "        [ 0.3514, -0.1685,  0.1196,  ..., -0.3544, -0.4076,  0.4999],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786]],\n",
      "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 63, 62, 59, 54, 51, 48, 47, 43, 41, 38,\n",
      "        30, 26, 20, 16, 14, 11,  9,  8,  7,  6,  5,  4,  2,  2,  1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([4635, 512])\n",
      "torch.Size([4635, 9956])\n",
      "torch.Size([4635, 9956]) torch.Size([4635])\n",
      "[-0.03789119  0.03869124  0.00109431 ... -0.03455511  0.02972135\n",
      "  0.00697916] 1\n",
      "torch.Size([64, 96, 300])\n",
      "torch.Size([64, 97, 300])\n",
      "PackedSequence(data=tensor([[ 1.6590e-01, -1.9365e-01, -3.6259e-01,  ...,  2.4085e-01,\n",
      "         -4.7934e-01,  7.0432e-04],\n",
      "        [ 8.0001e-01,  3.6170e-02,  4.9633e-01,  ...,  1.2851e-01,\n",
      "          2.1909e-01,  3.2591e-01],\n",
      "        [ 4.6888e-01, -4.8083e-01, -3.6344e-02,  ...,  4.6969e-01,\n",
      "         -1.5716e-01,  1.8557e-01],\n",
      "        ...,\n",
      "        [-1.2559e-01,  1.3630e-02,  1.0306e-01,  ..., -3.4224e-01,\n",
      "         -2.2394e-02,  1.3684e-01],\n",
      "        [ 3.5139e-01, -1.6855e-01,  1.1962e-01,  ..., -3.5437e-01,\n",
      "         -4.0756e-01,  4.9995e-01],\n",
      "        [ 7.0199e-02, -6.6328e-02, -5.2086e-01,  ..., -5.0042e-01,\n",
      "          4.8660e-01,  2.7863e-01]], device='cuda:0',\n",
      "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 63, 63, 63, 63, 62, 61, 59, 58, 56, 53, 52, 47, 43, 39,\n",
      "        37, 32, 24, 22, 19, 16, 14, 12, 11,  9,  6,  6,  4,  2,  2,  2,  2,  2,\n",
      "         1,  1,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([4722, 512])\n",
      "torch.Size([4722, 9956])\n",
      "torch.Size([4722, 9956]) torch.Size([4722])\n",
      "[-0.03475236 -0.01956256  0.02944243 ...  0.00836394  0.04155241\n",
      "  0.04279947] 1\n",
      "torch.Size([64, 98, 300])\n",
      "torch.Size([64, 99, 300])\n",
      "PackedSequence(data=tensor([[ 0.1786, -0.2364,  0.4676,  ...,  0.4463,  0.0368,  0.5560],\n",
      "        [ 0.2492,  0.1659, -0.2601,  ..., -0.1136,  0.3708, -0.0174],\n",
      "        [ 0.1536, -0.3760,  0.0010,  ..., -0.3605,  0.1116,  0.7969],\n",
      "        ...,\n",
      "        [ 0.1603,  0.0214, -0.0873,  ..., -0.4157,  0.2130,  0.2098],\n",
      "        [ 0.3514, -0.1685,  0.1196,  ..., -0.3544, -0.4076,  0.4999],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786]],\n",
      "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 63, 62, 62, 62, 62, 61, 57, 50, 48, 42, 39, 35,\n",
      "        33, 30, 30, 28, 24, 19, 16, 13, 10,  8,  5,  5,  5,  4,  4,  4,  3,  2,\n",
      "         2,  2,  2,  2,  1,  1,  1,  1]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([4738, 512])\n",
      "torch.Size([4738, 9956])\n",
      "torch.Size([4738, 9956]) torch.Size([4738])\n",
      "[-0.05348089 -0.00899864  0.01860371 ... -0.01430168  0.05101619\n",
      "  0.02862716] 1\n",
      "torch.Size([64, 91, 300])\n",
      "torch.Size([64, 92, 300])\n",
      "PackedSequence(data=tensor([[ 0.3350, -0.0830,  0.6078,  ...,  0.4766, -0.1875,  0.2682],\n",
      "        [ 0.6029,  0.1876, -0.4343,  ...,  0.3063,  0.1153,  0.2269],\n",
      "        [ 0.7264, -0.6354, -0.4283,  ...,  0.6410,  0.1276,  0.3282],\n",
      "        ...,\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786]],\n",
      "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 63, 63, 62, 62, 62, 58, 55, 51, 48, 44, 42, 36,\n",
      "        29, 29, 25, 21, 19, 17, 14, 14, 13, 11,  8,  8,  8,  7,  7,  7,  6,  5,\n",
      "         3]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([4737, 512])\n",
      "torch.Size([4737, 9956])\n",
      "torch.Size([4737, 9956]) torch.Size([4737])\n",
      "[-0.04845945  0.02442612  0.03366292 ... -0.02911043  0.01850647\n",
      " -0.00800366] 1\n",
      "torch.Size([64, 92, 300])\n",
      "torch.Size([64, 93, 300])\n",
      "PackedSequence(data=tensor([[ 0.2346, -0.1234, -0.2230,  ...,  0.0064,  0.1317,  0.0375],\n",
      "        [ 0.0292, -0.2688, -0.2403,  ..., -0.2560, -0.1785, -0.1280],\n",
      "        [-0.0454, -0.3486, -0.0602,  ...,  0.7354, -0.1892,  0.2920],\n",
      "        ...,\n",
      "        [ 0.3514, -0.1685,  0.1196,  ..., -0.3544, -0.4076,  0.4999],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786],\n",
      "        [ 0.0702, -0.0663, -0.5209,  ..., -0.5004,  0.4866,  0.2786]],\n",
      "       device='cuda:0', grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,\n",
      "        64, 64, 64, 64, 64, 64, 64, 62, 59, 59, 58, 56, 52, 50, 45, 42, 38, 36,\n",
      "        31, 26, 22, 17, 15, 14, 10,  9,  9,  7,  7,  7,  5,  5,  4,  4,  4,  3,\n",
      "         2,  2]), sorted_indices=None, unsorted_indices=None)\n",
      "torch.Size([4664, 512])\n",
      "torch.Size([4664, 9956])\n",
      "torch.Size([4664, 9956]) torch.Size([4664])\n",
      "[-0.03270366  0.00640432  0.01577147 ... -0.04465232 -0.00351119\n",
      " -0.01243391] 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4b693c1b5435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtotal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Set mini-batch dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/home/home-03/91/891/snagesh/Image_Captioning/data_loader.ipynb\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      7\u001b[0m    \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m    \"source\": [\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m\"import torchvision.models as models\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m    ]\n\u001b[1;32m     11\u001b[0m   },\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/datasets/coco.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadImgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \"\"\"\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "log_step = 50\n",
    "save_step = 200\n",
    "num_epochs = 25\n",
    "model_path = \"model3\"\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder.forward(images)\n",
    "        outputs = decoder.forward(features, captions, lengths)\n",
    "        print(outputs.shape, targets.shape)\n",
    "        print(outputs[1, :].cpu().detach().numpy(), targets[1].cpu().detach().numpy())\n",
    "#         print(np.unique(targets.cpu().detach().numpy()))\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % log_step == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                  .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "\n",
    "        # Save the model checkpoints\n",
    "        if (i+1) % save_step == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join(\n",
    "                model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
